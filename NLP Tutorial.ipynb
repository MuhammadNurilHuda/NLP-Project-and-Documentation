{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP ?\n",
    "NLP is a subfield of AI which enable computers to understand and process human language. The main task of NLP would be to program computers for analyzing and processing huge amount of natural language data \n",
    "\n",
    "NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine learning, and deep learning models. Together, these technologies enable computers to process human language in the form of text or voice data and to ‘understand’ its full meaning, complete with the speaker or writer’s intent and sentiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task\n",
    "Human language is filled with ambiguities that make it incredibly difficult to write software that accurately determines the intended meaning of text or voice data. Homonyms, homophones, sarcasm, idioms, metaphors, grammar and usage exceptions, variations in sentence structure—these just a few of the irregularities of human language that take humans years to learn, but that programmers must teach natural language-driven applications to recognize and understand accurately from the start, if those applications are going to be useful.\n",
    "\n",
    "Several NLP tasks break down human text and voice data in ways that help the computer make sense of what it's ingesting. Some of these tasks include the following:\n",
    "\n",
    "- **Speech recognition**, also called speech-to-text, is the task of reliably converting voice data into text data. Speech recognition is required for any application that follows voice commands or answers spoken questions. What makes speech recognition especially challenging is the way people talk—quickly, slurring words together, with varying emphasis and intonation, in different accents, and often using incorrect grammar.\n",
    "\n",
    "- **Part of speech tagging**, also called grammatical tagging, is the process of determining the part of speech of a particular word or piece of text based on its use and context. Part of speech identifies ‘make’ as a verb in ‘I can make a paper plane,’ and as a noun in ‘What make of car do you own?’\n",
    "\n",
    "- **Word sense disambiguation** is the selection of the meaning of a word with multiple meanings  through a process of semantic analysis that determine the word that makes the most sense in the given context. For example, word sense disambiguation helps distinguish the meaning of the verb 'make' in ‘make the grade’ (achieve) vs. ‘make a bet’ (place).\n",
    "\n",
    "- **Named entity recognition**, or NEM, identifies words or phrases as useful entities. NEM identifies ‘Kentucky’ as a location or ‘Fred’ as a man's name.\n",
    "\n",
    "- **Co-reference resolution** is the task of identifying if and when two words refer to the same entity. The most common example is determining the person or object to which a certain pronoun refers (e.g., ‘she’ = ‘Mary’),  but it can also involve identifying a metaphor or an idiom in the text  (e.g., an instance in which 'bear' isn't an animal but a large hairy person).\n",
    "\n",
    "- **Sentiment analysis** attempts to extract subjective qualities—attitudes, emotions, sarcasm, confusion, suspicion—from text.\n",
    "\n",
    "- **Natural language generation** is sometimes described as the opposite of speech recognition or speech-to-text; it's the task of putting structured information into human language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP tools and approaches\n",
    "\n",
    "**Python and the Natural Language Toolkit (NLTK)**\n",
    "\n",
    "The Python programing language provides a wide range of tools and libraries for attacking specific NLP tasks. Many of these are found in the Natural Language Toolkit, or NLTK, an open source collection of libraries, programs, and education resources for building NLP programs.\n",
    "\n",
    "The NLTK includes libraries for many of the NLP tasks listed above, plus libraries for subtasks, such as sentence parsing, word segmentation, stemming and lemmatization (methods of trimming words down to their roots), and tokenization (for breaking phrases, sentences, paragraphs and passages into tokens that help the computer better understand the text). It also includes libraries for implementing capabilities such as semantic reasoning, the ability to reach logical conclusions based on facts extracted from text.\n",
    "\n",
    "**Statistical NLP, machine learning, and deep learning**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Use Cases\n",
    "\n",
    "Natural language processing is the driving force behind machine intelligence in many modern real-world applications. Here are a few examples:\n",
    "\n",
    "- **Spam detection:** You may not think of spam detection as an NLP solution, but the best spam detection technologies use NLP's text classification capabilities to scan emails for language that often indicates spam or phishing. These indicators can include overuse of financial terms, characteristic bad grammar, threatening language, inappropriate urgency, misspelled company names, and more. Spam detection is one of a handful of NLP problems that experts consider 'mostly solved' (although you may argue that this doesn’t match your email experience).\n",
    "\n",
    "- **Machine translation:** Google Translate is an example of widely available NLP technology at work. Truly useful machine translation involves more than replacing words in one language with words of another.  Effective translation has to capture accurately the meaning and tone of the input language and translate it to text with the same meaning and desired impact in the output language. Machine translation tools are making good progress in terms of accuracy. A great way to test any machine translation tool is to translate text to one language and then back to the original. An oft-cited classic example: Not long ago, translating “The spirit is willing but the flesh is weak” from English to Russian and back yielded “The vodka is good but the meat is rotten.” Today, the result is “The spirit desires, but the flesh is weak,” which isn’t perfect, but inspires much more confidence in the English-to-Russian translation.\n",
    "\n",
    "- **Virtual agents and chatbots:** Virtual agents such as Apple's Siri and Amazon's Alexa use speech recognition to recognize patterns in voice commands and natural language generation to respond with appropriate action or helpful comments. Chatbots perform the same magic in response to typed text entries. The best of these also learn to recognize contextual clues about human requests and use them to provide even better responses or options over time. The next enhancement for these applications is question answering, the ability to respond to our questions—anticipated or not—with relevant and helpful answers in their own words.\n",
    "\n",
    "- **Social media sentiment analysis:** NLP has become an essential business tool for uncovering hidden data insights from social media channels. Sentiment analysis can analyze language used in social media posts, responses, reviews, and more to extract attitudes and emotions in response to products, promotions, and events–information companies can use in product designs, advertising campaigns, and more.\n",
    "Text summarization: Text summarization uses NLP techniques to digest huge volumes of digital text and create summaries and synopses for indexes, research databases, or busy readers who don't have time to read full text. The best text summarization applications use semantic reasoning and natural language generation (NLG) to add useful context and conclusions to summaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "\n",
    "1. **Sentence Segmentation**\n",
    "\n",
    "Sentence Segment is the first step for building the NLP pipeline. It breaks the paragraph into separate sentences.\n",
    "\n",
    "Example : Consider the following paragraph\n",
    "\n",
    "**Independence Day is one of the important festivals for every Indonesian citizen. It is celebrated on the 17th of August each year ever since the first President of Indonesia declared the proclamation. The day celebrates independence in the true sense.**\n",
    "\n",
    "Sentence Segment produces the following result: \n",
    "- \"Independence Day is one of the important festivals for every Indonesian citizen.\"\n",
    "- \"It is celebrated on the 17th of August each year ever since the first President of Indonesia declared the proclamation.\"\n",
    "- \"The day celebrates independence in the true sense.\"\n",
    "\n",
    "2. **Word Tokenization**\n",
    "\n",
    "Word Tokenizer is used to break the sentence into separate words or tokens.\n",
    "\n",
    "Example :\n",
    "\n",
    "**The winter is very cold.**\n",
    "\n",
    "Word Tokenizer generates the following result:\n",
    "\n",
    "\"The\", \"winter\", \"is\", \"very\", \"cold\", \".\"\n",
    "\n",
    "3. **Stemming**\n",
    "\n",
    "Stemming is used to normalize words into its base form or root form.\n",
    "\n",
    "Example : \n",
    "\n",
    "Intelligence, intelligent, and intelligently, all these words are originated with a single root word \"intelligen.\" In English, the word \"intelligen\" do not have any meaning.\n",
    "\n",
    "4. **Lemmatization**\n",
    "\n",
    "Lemmatization is quite similar to the Stemming. It is used to group different inflected forms of the word, called Lemma. The main difference between Stemming and lemmatization is that it produces the root word, which has a meaning.\n",
    "\n",
    "Example : In lemmatization, the words intelligence, intelligent, and intelligently has a root word intelligent, which has a meaning.\n",
    "\n",
    "5. **Stop Words**\n",
    "\n",
    "In English, there are a lot of words that appear very frequently like \"is\", \"and\", \"the\", and \"a\". NLP pipelines will flag these words as stop words. Stop words might be filtered out before doing any statistical analysis.\n",
    "\n",
    "Example: He **is a** good boy.\n",
    "\n",
    "6. **Dependency Parsing**\n",
    "\n",
    "Dependency Parsing is used to find that how all the words in the sentence are related to each other.\n",
    "\n",
    "7. **POS tags**\n",
    "\n",
    "POS stands for parts of speech, which includes Noun, verb, adverb, and Adjective. It indicates that how a word functions with its meaning as well as grammatically within the sentences. A word has one or more parts of speech based on the context in which it is used.\n",
    "\n",
    "Example: \"**Google**\" something on the Internet.\n",
    "\n",
    "8. **Named Entity Recognition (NER)**\n",
    "\n",
    "Named Entity Recognition (NER) is the process of detecting the named entity such as person name, movie name, organization name, or location.\n",
    "\n",
    "Example: **Steve Jobs** introduced iPhone at the Macworld Conference in San Francisco, California.\n",
    "\n",
    "9. **Chunking**\n",
    "\n",
    "Chunking is used to collect the individual piece of information and grouping them into bigger pieces of sentences.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Libraries\n",
    "\n",
    "**Scikit-learn:** It provides a wide range of algorithms for building machine learning models in Python.\n",
    "\n",
    "**Natural language Toolkit (NLTK):** NLTK is a complete toolkit for all NLP techniques.\n",
    "\n",
    "**Pattern:** It is a web mining module for NLP and machine learning.\n",
    "\n",
    "**TextBlob:** It provides an easy interface to learn basic NLP tasks like sentiment analysis, noun phrase extraction, or pos-tagging.\n",
    "\n",
    "**Quepy:** Quepy is used to transform natural language questions into queries in a database query language.\n",
    "\n",
    "**SpaCy:** SpaCy is an open-source NLP library which is used for Data Extraction, Data Analysis, Sentiment Analysis, and Text Summarization.\n",
    "\n",
    "**Gensim:** Gensim works with large datasets and processes data streams."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of NLP Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library from sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dummy train data\n",
    "\n",
    "x_train = [\n",
    "    \"the weather today is very cold\",\n",
    "    \"what are you going to do in this hot weather?\",\n",
    "    \"It is very hot today\",\n",
    "    \"The whole world is experiencing a financial crisis\",\n",
    "    \"The financial sector accounts for the largest profits in the USA\",\n",
    "    \"Banks are the most important sector for state finances\"\n",
    "            ]\n",
    "\n",
    "y_train = [\"WEATHER\", \"WEATHER\", \"WEATHER\", \"FINANCE\", \"FINANCE\", \"FINANCE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accounts' 'are' 'banks' 'cold' 'crisis' 'do' 'experiencing' 'finances'\n",
      " 'financial' 'for' 'going' 'hot' 'important' 'in' 'is' 'it' 'largest'\n",
      " 'most' 'profits' 'sector' 'state' 'the' 'this' 'to' 'today' 'usa' 'very'\n",
      " 'weather' 'what' 'whole' 'world' 'you']\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0]\n",
      " [1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "x_train_vector = vectorizer.fit_transform(x_train)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(x_train_vector.toarray())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That mean in the first sentence contain word \"cold\" 1, word \"is\" 1, etc.\n",
    "\n",
    "Now let's train the model with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the library\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(x_train_vector, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FINANCE'], dtype='<U7')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = ['Stocks greatly affect the finances of a country']\n",
    "\n",
    "x_test_vector = vectorizer.transform(x_test)\n",
    "\n",
    "clf.predict(x_test_vector)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already make a simple Text Classification !!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADDITION**\n",
    "\n",
    "In scikit-learn, fit, transform, and fit_transform are three methods used in the process of data preprocessing and modeling.\n",
    "\n",
    "- `fit` is used to calculate the necessary statistics on the training data. This is done by applying a particular algorithm to the data to learn its characteristics, and the resulting model is stored in memory.\n",
    "\n",
    "- `transform` is used to apply the same preprocessing steps to the new or unseen data. This step ensures that the new data is in the same format as the original data.\n",
    "\n",
    "- `fit_transform` is a combination of fit and transform. It first fits the model to the training data and then applies the transformation on the same data. This is a convenient way to apply both steps at once.\n",
    "\n",
    "In summary, `fit` is used to learn the characteristics of the data and create a model, `transform` is used to apply the learned model to new data, and `fit_transform` is used to learn the model and apply it to the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
